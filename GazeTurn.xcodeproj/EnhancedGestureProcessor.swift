//
//  EnhancedGestureProcessor.swift
//  GazeTurn
//
//  Created by Claude Code on 2025/1/15.
//

import Foundation
import Vision
import Combine

/// å¢å¼·çš„æ‰‹å‹¢è™•ç†å™¨ï¼Œæ•´åˆ AI å­¸ç¿’å¼•æ“
class EnhancedGestureProcessor: ObservableObject {
    
    // MARK: - Published Properties
    
    /// ç•¶å‰æª¢æ¸¬ä¿¡å¿ƒåº¦
    @Published var detectionConfidence: Double = 0.0
    
    /// æ‰‹å‹¢å“è³ªè©•åˆ†
    @Published var gestureQuality: GestureQuality = .unknown
    
    /// è™•ç†ç‹€æ…‹
    @Published var processingStatus: ProcessingStatus = .idle
    
    // MARK: - Components
    
    private let visionProcessor: VisionProcessor
    private let learningEngine: GestureLearningEngine?
    private var currentContext: GestureContext?
    
    /// æ˜¯å¦å•Ÿç”¨ AI å­¸ç¿’
    var enableLearning: Bool = true
    
    /// ç’°å¢ƒå…‰ç·šæª¢æ¸¬
    private var currentBrightness: Float = 0.8
    
    /// ç”¨æˆ¶è·é›¢ä¼°ç®—ï¼ˆåŸºæ–¼è‡‰éƒ¨å¤§å°ï¼‰
    private var estimatedDistance: Double = 50.0 // cm
    
    // MARK: - Statistics
    
    private var processingStats = ProcessingStatistics()
    
    // MARK: - Initialization
    
    init(
        visionProcessor: VisionProcessor,
        enableLearning: Bool = true
    ) {
        self.visionProcessor = visionProcessor
        self.enableLearning = enableLearning
        self.learningEngine = enableLearning ? GestureLearningEngine() : nil
        
        // å•Ÿç”¨è©³ç´°ç‰¹å¾µæå–
        visionProcessor.enableDetailedFeatures = true
    }
    
    // MARK: - Main Processing Pipeline
    
    /// è™•ç†å½±åƒå¹€ä¸¦åŸ·è¡Œå®Œæ•´çš„æ‰‹å‹¢è­˜åˆ¥æµç¨‹
    /// - Parameters:
    ///   - pixelBuffer: å½±åƒæ•¸æ“š
    ///   - instrumentMode: ç•¶å‰æ¨‚å™¨æ¨¡å¼
    /// - Returns: æ‰‹å‹¢è™•ç†çµæœ
    func processGesture(
        from pixelBuffer: CVPixelBuffer,
        mode: InstrumentMode
    ) -> GestureProcessingResult? {
        
        processingStatus = .processing
        
        // 1ï¸âƒ£ åŸ·è¡Œ Vision è™•ç†
        guard let result = visionProcessor.processFrameWithFeatures(pixelBuffer) else {
            processingStatus = .failed(reason: "æœªæª¢æ¸¬åˆ°è‡‰éƒ¨")
            processingStats.failedFrames += 1
            return nil
        }
        
        processingStats.processedFrames += 1
        
        // 2ï¸âƒ£ æ›´æ–°ç’°å¢ƒä¸Šä¸‹æ–‡
        updateContext(from: result, mode: mode)
        
        // 3ï¸âƒ£ è©•ä¼°æª¢æ¸¬å“è³ª
        let quality = evaluateGestureQuality(result: result)
        
        DispatchQueue.main.async {
            self.gestureQuality = quality
            self.detectionConfidence = Double(result.confidence)
        }
        
        // 4ï¸âƒ£ å¦‚æœå“è³ªä¸ä½³ï¼Œæä¾›åé¥‹
        if quality == .poor || quality == .veryPoor {
            processingStatus = .warning(message: getQualityFeedback(for: quality))
        } else {
            processingStatus = .success
        }
        
        // 5ï¸âƒ£ è¨˜éŒ„åˆ°å­¸ç¿’å¼•æ“ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if enableLearning, let context = currentContext {
            recordToLearningEngine(
                result: result,
                context: context,
                mode: mode
            )
        }
        
        return result
    }
    
    /// è™•ç†æ‰‹å‹¢äº‹ä»¶ä¸¦è¨˜éŒ„å­¸ç¿’æ•¸æ“š
    /// - Parameters:
    ///   - gestureType: æ‰‹å‹¢é¡å‹
    ///   - features: æ‰‹å‹¢ç‰¹å¾µ
    ///   - outcome: æ‰‹å‹¢çµæœ
    ///   - threshold: ä½¿ç”¨çš„é–¾å€¼
    func recordGestureEvent(
        type: GestureTrainingData.GestureType,
        features: GestureFeatures,
        outcome: GestureTrainingData.GestureOutcome,
        threshold: Double
    ) {
        guard enableLearning,
              let engine = learningEngine,
              let context = currentContext else {
            return
        }
        
        let trainingData = GestureTrainingData(
            gestureType: type,
            timestamp: Date(),
            context: context,
            features: features,
            outcome: outcome,
            threshold: threshold,
            confidence: detectionConfidence
        )
        
        engine.recordGestureData(trainingData)
        
        // æ›´æ–°çµ±è¨ˆ
        processingStats.gesturesRecorded += 1
    }
    
    /// ç²å–è‡ªé©æ‡‰é–¾å€¼
    /// - Parameters:
    ///   - parameter: åƒæ•¸åç¨±
    ///   - defaultValue: é è¨­å€¼
    /// - Returns: èª¿æ•´å¾Œçš„é–¾å€¼
    func getAdaptiveThreshold(for parameter: String, defaultValue: Double) -> Double {
        guard enableLearning,
              let engine = learningEngine,
              let context = currentContext else {
            return defaultValue
        }
        
        return engine.getAdaptiveThreshold(for: parameter, context: context)
    }
    
    // MARK: - Quality Evaluation
    
    /// è©•ä¼°æ‰‹å‹¢æª¢æ¸¬å“è³ª
    /// - Parameter result: Vision è™•ç†çµæœ
    /// - Returns: æ‰‹å‹¢å“è³ªç­‰ç´š
    private func evaluateGestureQuality(result: GestureProcessingResult) -> GestureQuality {
        var score: Double = 0.0
        
        // 1. è‡‰éƒ¨æª¢æ¸¬ä¿¡å¿ƒåº¦ (40%)
        let confidenceScore = Double(result.confidence) * 0.4
        score += confidenceScore
        
        // 2. è¿½è¹¤å“è³ª (20%)
        let trackingScore = evaluateTrackingQuality(result.features.trackingQuality) * 0.2
        score += trackingScore
        
        // 3. ç’°å¢ƒå…‰ç·š (20%)
        let lightingScore = evaluateLightingCondition() * 0.2
        score += lightingScore
        
        // 4. è‡‰éƒ¨è§’åº¦ (20%)
        let angleScore = evaluateFaceAngle(result.features) * 0.2
        score += angleScore
        
        // æ ¹æ“šåˆ†æ•¸è¿”å›å“è³ªç­‰ç´š
        switch score {
        case 0.9...1.0: return .excellent
        case 0.7..<0.9: return .good
        case 0.5..<0.7: return .fair
        case 0.3..<0.5: return .poor
        default: return .veryPoor
        }
    }
    
    private func evaluateTrackingQuality(_ quality: FaceTrackingQuality) -> Double {
        switch quality {
        case .high:
            return 1.0
        case .medium:
            return 0.7
        case .low:
            return 0.4
        }
    }
    
    private func evaluateLightingCondition() -> Double {
        switch currentBrightness {
        case 0.8...1.0: return 1.0
        case 0.6..<0.8: return 0.8
        case 0.4..<0.6: return 0.6
        case 0.2..<0.4: return 0.4
        default: return 0.2
        }
    }
    
    private func evaluateFaceAngle(_ features: ExtractedFaceFeatures) -> Double {
        // ç†æƒ³æƒ…æ³ï¼šè‡‰éƒ¨æ­£å°ç›¸æ©Ÿ
        let yawDegrees = abs(features.yaw * 180.0 / .pi)
        let pitchDegrees = abs(features.pitch * 180.0 / .pi)
        
        // åé›¢è§’åº¦è¶Šå¤§ï¼Œåˆ†æ•¸è¶Šä½
        let yawScore = max(0, 1.0 - yawDegrees / 45.0)
        let pitchScore = max(0, 1.0 - pitchDegrees / 30.0)
        
        return (yawScore + pitchScore) / 2.0
    }
    
    private func getQualityFeedback(for quality: GestureQuality) -> String {
        switch quality {
        case .veryPoor:
            return "æª¢æ¸¬å“è³ªå¾ˆä½ï¼Œè«‹ç¢ºä¿ï¼š1) å…‰ç·šå……è¶³ 2) è‡‰éƒ¨æ­£å°ç›¸æ©Ÿ 3) è·é›¢é©ä¸­"
        case .poor:
            return "æª¢æ¸¬å“è³ªè¼ƒä½ï¼Œå»ºè­°èª¿æ•´å…‰ç·šæˆ–è‡‰éƒ¨è§’åº¦"
        case .fair:
            return "æª¢æ¸¬å“è³ªä¸€èˆ¬ï¼Œå¯ç¹¼çºŒä½¿ç”¨ä½†å»ºè­°å„ªåŒ–ç’°å¢ƒ"
        default:
            return ""
        }
    }
    
    // MARK: - Context Management
    
    /// æ›´æ–°ç•¶å‰æ‰‹å‹¢ä¸Šä¸‹æ–‡
    private func updateContext(from result: GestureProcessingResult, mode: InstrumentMode) {
        // ä¼°ç®—ç”¨æˆ¶è·é›¢ï¼ˆåŸºæ–¼è‡‰éƒ¨å¤§å°ï¼‰
        let faceSize = result.faceObservation.boundingBox.width
        estimatedDistance = estimateDistance(from: faceSize)
        
        // æ›´æ–°ç’°å¢ƒå…‰ç·šï¼ˆé€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›å¯å¾ç›¸æ©Ÿç²å–ï¼‰
        // å¯ä»¥é€šéåˆ†æ pixelBuffer çš„å¹³å‡äº®åº¦ä¾†ç²å–
        currentBrightness = 0.8 // æš«æ™‚ä½¿ç”¨å›ºå®šå€¼
        
        currentContext = GestureContext(
            instrumentType: mode.instrumentType,
            lightingCondition: .from(brightness: currentBrightness),
            userDistance: estimatedDistance,
            sessionDuration: Date().timeIntervalSince(processingStats.sessionStartTime),
            practiceMode: false
        )
    }
    
    /// ä¼°ç®—ç”¨æˆ¶è·é›¢
    private func estimateDistance(from faceSize: CGFloat) -> Double {
        // ç°¡åŒ–çš„è·é›¢ä¼°ç®—
        // å‡è¨­æ¨™æº–è‡‰éƒ¨å¯¬åº¦ç´„ 15cmï¼Œä½¿ç”¨ç›¸ä¼¼ä¸‰è§’å½¢åŸç†
        // é€™æ˜¯ä¸€å€‹ç²—ç•¥ä¼°ç®—ï¼Œå¯¦éš›æ‡‰è©²é€šéæ ¡æº–ç²å¾—æ›´æº–ç¢ºçš„å€¼
        
        let referenceFaceWidth: Double = 15.0 // cm
        let referenceFaceSize: CGFloat = 0.3 // åœ¨ 30cm æ™‚çš„æ¨™æº–åŒ–å¤§å°
        
        guard faceSize > 0 else { return 100.0 }
        
        let distance = (referenceFaceWidth * Double(referenceFaceSize)) / Double(faceSize)
        return min(max(distance, 20.0), 200.0) // é™åˆ¶åœ¨ 20-200cm ç¯„åœ
    }
    
    // MARK: - Learning Integration
    
    /// è¨˜éŒ„æ•¸æ“šåˆ°å­¸ç¿’å¼•æ“
    private func recordToLearningEngine(
        result: GestureProcessingResult,
        context: GestureContext,
        mode: InstrumentMode
    ) {
        // å°‡ Vision ç‰¹å¾µè½‰æ›ç‚ºå­¸ç¿’å¼•æ“çš„ç‰¹å¾µæ ¼å¼
        let features = convertToGestureFeatures(
            visionFeatures: result.features,
            context: context
        )
        
        // é€™è£¡è¨˜éŒ„ä¸€å€‹è§€å¯Ÿæ¨£æœ¬ï¼ˆä¸æ˜¯å¯¦éš›æ‰‹å‹¢ï¼‰
        // å¯¦éš›æ‰‹å‹¢æœƒåœ¨æª¢æ¸¬åˆ°æ™‚ç”± recordGestureEvent è¨˜éŒ„
    }
    
    /// è½‰æ› Vision ç‰¹å¾µåˆ°å­¸ç¿’å¼•æ“æ ¼å¼
    private func convertToGestureFeatures(
        visionFeatures: ExtractedFaceFeatures,
        context: GestureContext
    ) -> GestureFeatures {
        return GestureFeatures(
            eyeAspectRatio: visionFeatures.eyeAspectRatio,
            blinkDuration: 0.0, // éœ€è¦å¾æ™‚åºæ•¸æ“šè¨ˆç®—
            blinkVelocity: 0.0, // éœ€è¦å¾æ™‚åºæ•¸æ“šè¨ˆç®—
            headYaw: visionFeatures.yaw,
            headPitch: visionFeatures.pitch,
            headRoll: visionFeatures.roll,
            headMovementVelocity: 0.0, // éœ€è¦å¾æ™‚åºæ•¸æ“šè¨ˆç®—
            faceConfidence: Double(detectionConfidence),
            eyeOpenness: (visionFeatures.leftEyeOpenness + visionFeatures.rightEyeOpenness) / 2.0,
            mouthCurvature: visionFeatures.mouthCurvature ?? 0.0,
            timeSinceLastGesture: 0.0, // éœ€è¦è¿½è¹¤
            gestureFrequency: 0.0, // éœ€è¦çµ±è¨ˆ
            ambientLight: currentBrightness,
            deviceMotion: 0.0 // éœ€è¦å¾è¨­å‚™é‹å‹•æ„Ÿæ¸¬å™¨ç²å–
        )
    }
    
    // MARK: - Statistics and Diagnostics
    
    /// ç²å–è™•ç†çµ±è¨ˆä¿¡æ¯
    func getProcessingStatistics() -> String {
        var stats = visionProcessor.getProcessingStats()
        stats += "\n\n"
        stats += processingStats.description
        
        if let engine = learningEngine {
            stats += "\n\n"
            stats += "Learning Engine:\n"
            stats += "- Accuracy: \(String(format: "%.1f", engine.recentAccuracy * 100))%\n"
            stats += "- Adaptation Progress: \(String(format: "%.1f", engine.adaptationProgress * 100))%"
        }
        
        return stats
    }
    
    /// é‡ç½®çµ±è¨ˆæ•¸æ“š
    func resetStatistics() {
        visionProcessor.resetStats()
        processingStats = ProcessingStatistics()
    }
    
    /// ç²å–å­¸ç¿’å¼•æ“å»ºè­°
    func getLearningRecommendations() -> [String] {
        return learningEngine?.getPersonalizedRecommendations() ?? []
    }
    
    /// å°å‡ºå­¸ç¿’æ•¸æ“š
    func exportLearningData() -> Data? {
        return learningEngine?.exportLearningData()
    }
    
    /// åŒ¯å…¥å­¸ç¿’æ•¸æ“š
    func importLearningData(_ data: Data) -> Bool {
        return learningEngine?.importLearningData(data) ?? false
    }
}

// MARK: - Supporting Types

/// æ‰‹å‹¢å“è³ªç­‰ç´š
enum GestureQuality {
    case excellent  // å„ªç§€
    case good       // è‰¯å¥½
    case fair       // ä¸€èˆ¬
    case poor       // è¼ƒå·®
    case veryPoor   // å¾ˆå·®
    case unknown    // æœªçŸ¥
    
    var displayName: String {
        switch self {
        case .excellent: return "å„ªç§€"
        case .good: return "è‰¯å¥½"
        case .fair: return "ä¸€èˆ¬"
        case .poor: return "è¼ƒå·®"
        case .veryPoor: return "å¾ˆå·®"
        case .unknown: return "æœªçŸ¥"
        }
    }
    
    var emoji: String {
        switch self {
        case .excellent: return "ğŸŒŸ"
        case .good: return "âœ…"
        case .fair: return "âš ï¸"
        case .poor: return "âš ï¸"
        case .veryPoor: return "âŒ"
        case .unknown: return "â“"
        }
    }
}

/// è™•ç†ç‹€æ…‹
enum ProcessingStatus {
    case idle
    case processing
    case success
    case warning(message: String)
    case failed(reason: String)
    
    var displayMessage: String {
        switch self {
        case .idle: return "å°±ç·’"
        case .processing: return "è™•ç†ä¸­..."
        case .success: return "æˆåŠŸ"
        case .warning(let message): return "è­¦å‘Š: \(message)"
        case .failed(let reason): return "å¤±æ•—: \(reason)"
        }
    }
}

/// è™•ç†çµ±è¨ˆ
struct ProcessingStatistics: CustomStringConvertible {
    var sessionStartTime: Date = Date()
    var processedFrames: Int = 0
    var failedFrames: Int = 0
    var gesturesRecorded: Int = 0
    
    var successRate: Double {
        guard processedFrames > 0 else { return 0 }
        return Double(processedFrames - failedFrames) / Double(processedFrames) * 100
    }
    
    var description: String {
        let sessionDuration = Date().timeIntervalSince(sessionStartTime)
        return """
        Processing Statistics:
        - Session Duration: \(String(format: "%.1f", sessionDuration))s
        - Processed Frames: \(processedFrames)
        - Failed Frames: \(failedFrames)
        - Success Rate: \(String(format: "%.1f", successRate))%
        - Gestures Recorded: \(gesturesRecorded)
        """
    }
}
